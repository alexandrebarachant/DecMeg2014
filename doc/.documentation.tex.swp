\documentclass[11pt,a4paper]{article}
\usepackage[top=2in, bottom=1.5in, left=1in, right=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\title{\textbf{MEG decoding using Riemannian Geometry and Unsupervised classification.}}
\author{Alexandre Barachant \\ \\ Location : Grenoble, France \\ Email : alexandre.barachant@gmail.com }
\date{}
\newcommand{\R}{\mathbb{R}}

\newcommand{\change}{\textcolor{blue}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}} 
\begin{document}

\maketitle

\section{Summary}
The solution is composed by two classification steps. The first one is supervised and use data from the training subjects to build a generic model. This generic model will be applied on each test subjects to obtain a first estimation of the test labels. The second step is unsupervised and is applied independently on each test subjects. It will use the labels from the first step as an initialization of an iterative unsupervised algorithm, similar to a k-means clustering. 

These two steps are build upon methods originally devoted to classification of P300 Evoked potential for EEG Brain-Computer Interfaces (BCI) :
\begin{itemize}
\item A dedicated \textbf{Spatial filtering} is applied on the data in order to increase the signal to noise ratio and reduce the dimensionality of the signal.
\item Special form of the \textbf{covariance matrices} of the trials are used as features, and manipulated with tools from \textbf{Riemannian Geometry.} Indeed, covariance matrices are Symmetric and Positive-Definite Matrices (SPD), and therefore belong to a Riemannian manifold. A dedicated metric should be used in order to take into account the structure of the manifold.
\end{itemize}

\section{Introduction}
It is well known that EEG and MEG signals are very specific to each subject. Indeed, the organization and orientation of the cortical dipoles is slightly different for each individual. As a result, establishing a generic model, which gives high classification performance, is a very difficult task. With the exception of some special cases, a subject-specific model is always better than a generic model.

....

\section{Method}
\subsection{preprocessing}
The original data consist in MEG recording on 306 channels sampled at 250Hz. The duration of the signal is 1.5s, and a stimulus is presented at 0.5s. The first step of preprocessing is to discard the first 0.5 second of the signal, resulting in a 1s trial. The second step is to apply a 5-order Butterworth band-pass filter between 1 and 20 Hz.
\subsection{Spatial Filtering}
For each class, a set of 4 spatial filters are build in order to enhance the signal to noise ratio of the evoked potential. Therefore, the resulting signal is composed by $2\times 4 = 8 $ virtual channels. The spatial filters are estimated using an algorithm derived from the xDawn algorithm~\cite{xdawn}.
Let $\mathbf{X}_i \in \Re^{C \times N}$ denotes a trial of index $i$, with $C$ the number of channels and $N$ the number of time samples, and $y_i$ the class of this trial. Let denotes $\mathbf{P}^{(k)}$ the average trial of the class $k$ : 
\begin{equation}
\mathbf{P}^{(k)} = \frac{1}{\vert\mathcal{I}^{(k)} \vert} \sum_{i\in\mathcal{I}^{(k)}}\mathbf{X}_i,
\end{equation}
where $\mathcal{I}^{(k)}$ is the set of indices of the trials belonging to the class $k$, i.e. $\mathcal{I}^{(k)} = \lbrace i \; \vert \; y_i = k \rbrace$. Let $\mathbf{X}$ be the matrix representing the whole signal, build by concatenation of all the trials (from both classes). 

In this work, a spatial filter is a vector $\mathbf{w} \in \Re^{C \times 1}$. The spatial filter is estimated in order to increase the signal to noise ratio of a given class, i.e. for the class $k$ we have : 
\begin{equation}
\mathbf{w}^* = \argmax_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{P}^{(k)} {\mathbf{P}^{(k)}}^T \mathbf{w}}{\mathbf{w}^T\mathbf{X} \mathbf{X}^T  \mathbf{w}}.
\end{equation}
This Equation is a generalized Rayleigh quotient, the solutions can be found using an eigenvalue-eigenvector decomposition of the matrix $[(\mathbf{P}^{(k)} {\mathbf{P}^{(k)}}^T)(\mathbf{X} \mathbf{X}^T)^{-1}]$. This will give a total of $C$ solution, ranked by the value of the eigenvalues. For each class, only the 4 best spatial filters, corresponding to the 4 highest eigenvalue, are selected. 

Let denotes by $\mathbf{W}^{(k)} \in \Re^{C \times 4 }$ the selected spatial filters for the class $k$. Since we have two classes, the total number of spatial filters is 8. The spatial filters could be aggregated in a single matrix $\mathbf{W} = [ \mathbf{W}^{(0)} , \mathbf{W}^{(1)} ] \in \Re^{C \times 8}$. Then, the spatial filtering operation is simply the linear projection of the trial by the matrix $\mathbf{W}$ : 
\begin{equation}
\mathbf{Z}_i = \mathbf{W}^T \mathbf{X}_i
\end{equation}
Note that this step is supervised.
\subsection{Features}
Covariance matrices are used as feature. A special estimation of the covariance matrix is used in order to take into account the shape of the evoked potentials. First, we build a new trial $\tilde{\mathbf{Z}}_i \in \Re^{16 \times T}$ by the concatenation of the spatially filtered average evoked potential $\mathbf{P}^{(k)}$ and the spatially filtered trial $\mathbf{Z}_i$: 
\begin{equation}
\tilde{\mathbf{Z}}_i =  \left[ 
\begin{array}{c}
{\mathbf{W}^{(0)}}^T \mathbf{P}^{(0)}\\ 
{\mathbf{W}^{(1)}}^T \mathbf{P}^{(1)} \\
\mathbf{Z}_i
\end{array} \right].
\end{equation}
Then, we simply estimate the spatial covariance matrix $\mathbf{\Sigma}_i \in \Re^{16 \times 16}$ form the new trial $\tilde{\mathbf{Z}}_i$ : 

\begin{equation}
\mathbf{\Sigma}_i  = \frac{1}{N} \tilde{\mathbf{Z}}_i \tilde{\mathbf{Z}}_i^T
\end{equation}

Note that the features are not in the usual form of vectors, they are matrices. We can not simply vectorize these matrices, because we want to keep their special structure (SPD). Instead, we will use tool from Riemannian Geometry to manipulate them.

\subsection{Riemannian Geometry}
\paragraph{Riemannian Distance :}
For two covariance matrix $(\mathbf{\Sigma}_1$ and $\mathbf{\Sigma}_2) $, the Riemannian distance according to information geometry is given by~\cite{r}
\begin{equation}
\label{eq:Rgeodistance}
\delta_R(\mathbf{\Sigma}_1,\mathbf{\Sigma}_2) 
= 
\Vert \mathrm{log} \left( \mathbf{\Sigma}_1^{-1/2} \mathbf{\Sigma}_2 \mathbf{\Sigma}_1^{-1/2} \right) \Vert_F
=
\left[ \sum_{c=1}^{C} \log^2 \lambda_c \right]^{1/2},
\end{equation}
where $\lambda_c, c=1\ldots C$ are the real eigenvalues 
of $\mathbf{\Sigma}_1^{-1/2} \mathbf{\Sigma}_2 \mathbf{\Sigma}_1^{-1/2}$ and $C$ the number of electrodes.
 Note that this distance is also called \emph{Affine-invariant}~\cite{arsigny2007geometric}
This distance has two important properties of invariance. First, the distance is invariant by inversion, i.e., 
\begin{equation}
\label{eq:invariance1}
\delta_R(\mathbf{\Sigma}_1,\mathbf{\Sigma}_2) = \delta_R(\mathbf{\Sigma}^{-1}_1,\mathbf{\Sigma}^{-1}_2).
\end{equation}

Second, the distance is invariant by congruent transformation, meaning that for any invertible matrix $\mathbf{V} \in \mathcal{G}l(C)$ we have
\begin{equation}
\label{eq:invariance2}
\delta_R(\mathbf{\Sigma}_1,\mathbf{\Sigma}_2) = \delta_R(\mathbf{V}^T\mathbf{\Sigma}_1\mathbf{V},\mathbf{V}^T\mathbf{\Sigma}_2\mathbf{V}).
\end{equation}
\paragraph{Riemannian Mean :}
The Riemannian geometric mean of $I$ covariance matrices (denoted by $\mathfrak{G}(.)$), also called Fr\'echet mean, is defined as the matrix minimizing the sum of the squared Riemannian distances~\cite{pennec2006riemannian}, i.e.,

\begin{equation}
\mathfrak{G} \left( \mathbf{\Sigma}_1,\ldots,\mathbf{\Sigma}_I \right) = \argmin_{\mathbf{\Sigma}} 
\sum_{i=1}^{I} 
\delta_R^2 \left( \mathbf{\Sigma},\mathbf{\Sigma}_i \right).
\label{eq:geo_mean}
\end{equation}
There is no closed form expression for this mean, however a gradient descent in the manifold can be used in order to find the solution~\cite{pennec2006riemannian}. A matlab implementation is provided in a Matlab toolbox\footnote{http://github.com/alexandrebarachant/covariancetoolbox}.
Note that the geometric mean inherits the properties of invariance from the distance.

\paragraph{Tangent Space :} The tangent space is a local approximation of the manifold.
...

\subsection{Generic Model}
A generic model is build in order to achieve the best classification accuracy in Leave-One-Subject-Out cross-validation (LOSO-CV). The test labels obtained with the generic model will be usedas an initialisation for the unsupervised classification step. This generic model is build as follow : 
\begin{itemize}
\item For each of the 16 training subjects, an set of 8 spatial filters are trained. The spatial filters are applied on the 16 subjects. Special form covariances matrices are estimated and projected in the tangent space. Thus, for each subject, we have a feature subspace of dimension [$ 136 \times $ Total Number of Trial] = $136 \times 9414$ (approximately 588 trial per subjects)
\item The 16 individual features subspace are aggregated to build a new feature space. The dimension of this feature space is $16*136 \times 9414 = 2176 \times 9414$
\item A Regularized logistic regression is trained on this feature space (function \emph{lasso }from matlab)
\end{itemize}
This generic model gives a accuracy of 69.7\% in LOSO-CV, with a public leaderboard score of 0.698 (private 0.65)

\subsection{Unsupervised training}
For each test subjects, an unsupervised algorithm is applied, using only data from the subject at hand, and initialized with the labels from the generic model. The procedure is inspired from the k-means clustering algorithm, in the Riemannian manifold. The procedure is iterative, and stop when the convergence is reached or after 10 iterations.
Lets denote by $y_{i,n}$ the label of the trial $i$ at the iteration $n$. The procedure is the following :
\begin{enumerate}
\item Train spatial filters given the labels $y_{i,n}$.
\item Apply the spatial filters on the trials.
\item Estimate special form covariance matrices.
\item For each class, estimate the mean covariance matrix from Eq.(\ref{eq:geo_mean}). We obtain two mean covariance matrix : $\mathbf{\Sigma}^{(0)}$ and $\mathbf{\Sigma}^{(1)}$.
\item Classify each trial according to the Riemannian distance from Eq.(\ref{eq:Rgeodistance}) to obtain new labels : 
\begin{equation}
 	 y_{i,(n+1)} = \left\{ \begin{array}{rl}
0 & \text{if} \:  \delta_R(\mathbf{\Sigma}^{(0)},\mathbf{\Sigma}_i) < \delta_R(\mathbf{\Sigma}^{(1)},\mathbf{\Sigma}_i)\\
1 & \text{otherwise}
\end{array} \right.
\end{equation} 
\item Stop if $y_{i,(n+1)}= y_{i,n}$ or if $n>10$. Restart to step 1 with the new labels $y_{i,(n+1)}$ if not.
\end{enumerate}

This second classification step allow to reach 75.8\% of accuracy in LOSO-CV, with a public leaderboard score of 0.774 (0.755 private). There is a risk that the algorithm does not converge for some subject. The critical part is the initialization and it is important that the generic model gives the best possible initialization.
Note that if the labels are know, this procedure could be applied in a supervised way (or semi-supervised way), with only one iteration.

\section{Results}
The results are given in Table 1. Single stands for the method described in section 3.6 applied in 6-fold CV using data of a single subject with known labels. This represent the maximum achievable accuracy if we have training data with labels for the subject at hand. Generic is the method described in section 3.5, i.e the generic model only. Gen + Unsup is the combination of 3.5 and 3.6, i.e. the winning submission. Note that for two subjects, marked with a star, the unsupervised method did not converge, leading to a decrease in accuracy.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
Subject & Single & Generic    & Gen + Unsup     \\
\hline
1       & 87.0     & 75.5     & 64.3*   \\
2       & 82.2     & 67.5     & 72.7    \\
3       & 82.8     & 61.0     & 63.3   \\
4       & 90.7     & 75.9     & 86.5   \\
5       & 81.5     & 68.4     & 74.2   \\
6       & 83.8     & 67.6     & 73.8   \\
7       & 88.7     & 69.7     & 79.9   \\
8       & 88.8     & 68.2     & 76.6   \\
9       & 89.9     & 69.5     & 79.6   \\
10      & 86.7     & 71.5     & 78.9   \\
11      & 76.0     & 65.5     & 59.9*   \\
12      & 85.4     & 78.5     & 83.7   \\
13      & 85.8     & 68.2     & 73.4   \\
14      & 91.6     & 74.8     & 87.0   \\
15      & 91.3     & 73.9     & 87.0   \\
16      & 87.1     & 58.8     & 72.5   \\
\hline
\hline
\textbf{Mean (std) }   & \textbf{86.2 } (4.2) & \textbf{69.7} (5.3)& \textbf{75.8} (8.3) \\
\hline
public LB & NA & 69.8 & 77.8\\
private LB & NA & 65.0 & 75.5\\
\hline
\hline
\end{tabular}
\caption{Classification accuracy for the different methods. The $*$ corresponds to the subjects where the usupervised method did not converge.}
\end{center}
\end{table}

\section{Function and Code}
The code is written in Matlab and is available here : https://github.com/alexandrebarachant/DecMeg2014

There is two scripts and 4 functions.

\end{document}
